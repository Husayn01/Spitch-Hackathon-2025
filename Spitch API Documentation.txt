Developer Platform
the most accurate voice AI platform for Africa.
Developer quickstart
Make your first API call, learn about the Spitch API.


Copy
from spitch import Spitch
client = Spitch(api_key="YOUR-API-KEY")

response = client.speech.transcribe(
    content=open("file.wav", "rb"),
    language="en",
    model="mansa_v1"
)

print(response.text)


Checkout our models
mansa
Mansa v1
the most versatile ASR model for African languages.


Start building
Audio Transcription
create a web application that transcribes audio files in seconds with Spitch!
Multilingual Chatbot
create chatbots that converse in multiple languages using translation API.
Voice AI Agent
use our API to build voice agents with LiveKit.
Real-time Transcription
implement live speech-to-text transcription for online meetings and webinars.
Was this page helpful?


Yes

No
Quickstart
Start building with Spitch in 2 minutes or less!
Next
Get Started
Quickstart

Copy page

Start building with Spitch in 2 minutes or less!

Transcribe audio
Generate audio
Translate text
Generate diacritics
Get an API Key

Creating a Spitch API key
Creating a Spitch API key

Screenshot of API key created
API key has been created

Create your API key at Spitch Studio. This will be used to securely access the API.
Install your library

Our official Python and Node SDKs allow you easily use the API. You can also install dotenv so that your API keys are set properly.
Install the official Spitch Python SDK


Copy
pip install spitch
pip install python-dotenv
More info on installation can be found on Installation
Set environment variable

Set your environment variable depending on your OS
MacOS/Linux


Copy
export SPITCH_API_KEY="your_api_key_here"
Alternatively, you can set it in your .env file

Copy
SPITCH_API_KEY=<your_api_key>
Write your script

app.py


Copy
from spitch import Spitch
client = Spitch(api_key="YOUR-API-KEY")

response = client.speech.transcribe(
    content=open("file.wav", "rb"),
    language="en",
    model="mansa_v1"
)

print(response.text)
Run the code



Copy
python app.py
You should see the text displayed on console.
Was this page helpful?
Quickstart

Copy page

Start building with Spitch in 2 minutes or less!

Transcribe audio
Generate audio
Translate text
Generate diacritics
Get an API Key

Creating a Spitch API key
Creating a Spitch API key

Screenshot of API key created
API key has been created

Create your API key at Spitch Studio. This will be used to securely access the API.
Install your library

Our official Python and Node SDKs allow you easily use the API. You can also install dotenv so that your API keys are set properly.
Install the official Spitch Python SDK


Copy
pip install spitch
pip install python-dotenv
More info on installation can be found on Installation
Set environment variable

Set your environment variable depending on your OS
MacOS/Linux


Copy
export SPITCH_API_KEY="your_api_key_here"
Alternatively, you can set it in your .env file

Copy
SPITCH_API_KEY=<your_api_key>
Write your script

app.py


Copy
from spitch import Spitch
client = Spitch(api_key="YOUR-API-KEY")

with open("audio.mp3", "wb") as f:
    response = client.speech.generate(
        text="I'll be in the backyard of the Ikorodu house.",
        language="en",
        voice="lina"
    )
    f.write(response.read())                    
Run the code



Copy
python app.py
You should see the text displayed on console.
Quickstart

Copy page

Start building with Spitch in 2 minutes or less!

Transcribe audio
Generate audio
Translate text
Generate diacritics
Get an API Key

Creating a Spitch API key
Creating a Spitch API key

Screenshot of API key created
API key has been created

Create your API key at Spitch Studio. This will be used to securely access the API.
Install your library

Our official Python and Node SDKs allow you easily use the API. You can also install dotenv so that your API keys are set properly.
Install the official Spitch Python SDK


Copy
pip install spitch
pip install python-dotenv
More info on installation can be found on Installation
Set environment variable

Set your environment variable depending on your OS
MacOS/Linux


Copy
export SPITCH_API_KEY="your_api_key_here"
Alternatively, you can set it in your .env file

Copy
SPITCH_API_KEY=<your_api_key>
Write your script

app.py


Copy
from spitch import Spitch
client = Spitch(api_key="YOUR-API-KEY")

translation = client.text.translate(
text="I'll be in the backyard of the Ikorodu house.",
source="en",
target="yo",
)

print(translation.text)
Run the code



Copy
python app.py
You should see the text displayed on console.
Was this page helpful?
Start building with Spitch in 2 minutes or less!

Transcribe audio
Generate audio
Translate text
Generate diacritics
Get an API Key

Creating a Spitch API key
Creating a Spitch API key

Screenshot of API key created
API key has been created

Create your API key at Spitch Studio. This will be used to securely access the API.
Install your library

Our official Python and Node SDKs allow you easily use the API. You can also install dotenv so that your API keys are set properly.
Install the official Spitch Python SDK


Copy
pip install spitch
pip install python-dotenv
More info on installation can be found on Installation
Set environment variable

Set your environment variable depending on your OS
MacOS/Linux


Copy
export SPITCH_API_KEY="your_api_key_here"
Alternatively, you can set it in your .env file

Copy
SPITCH_API_KEY=<your_api_key>
Write your script

app.py


Copy
import os
from spitch import Spitch

client = Spitch(api_key="YOUR-API-KEY")
response = client.text.tone_mark(
    language="yo",
    text="Bawo ni ololufe mi?"
)
print(response.text)   # B√°wo ni ol√≥l√πf·∫πÃÅ mi?
Run the code



Copy
python app.py
You should see the text displayed on console.
Was this page helpful?


Get Started
Libraries

Copy page

Here are the Spitch libraries to get started with our SDKs

Learn how to set up a local development environment on your system to use Spitch API. We currently support:
Python
JavaScipt
‚Äã
Accessing your API Key
Log on to the developer portal to create your API Key. You can store your API Key in a .env file, or use it directly in your code.
.env

Copy
SPITCH_API_KEY = "YOUR_API_KEY"
‚Äã
Installation
Python


Copy
pip install spitch
‚Äã
Usage
Test your installation by running the sample code below for text translation.
Python


Copy
from spitch import Spitch

client = Spitch(api_key="YOUR-API-KEY")
translation = client.text.translate(
    text="Hey my dear friend, how are you doing?",
    source="en",
    target="ha",
)
print(translation.text)
‚Äã
API Reference
For full details on all available methods and options, check out the API Reference.
Was this page helpful?
Get Started
Models

Copy page

Here‚Äôs what you need to know about our models!

‚Äã
Mansa.v1

Mansa is a 1.4B parameter ASR model optimized for African names and concepts, offering high transcription accuracy and low-latency performance.
Some of the key features are:
African named entity recognition in English contexts
Custom spelling guidance for names and specialized terms.
Sentence or Word-level timestamps for audio up to 30 minutes (25MB)
Mansa currently supports English only. If you try to use it with any other language, the API will return a 400 error.
‚Äã
Getting Started
Before you can try out our new model, you need to make sure that you‚Äôre using the latest version of our SDK.

Python SDK

Node SDK

Copy
pip install spitch>=1.34.0
Ready to give it a spin? Use the code sample below to get started.
Python


Copy
from spitch import Spitch
client = Spitch(api_key="YOUR-API-KEY")

response = client.speech.transcribe(
    content=open("YOUR-AUDIO-FILE", "rb"), 
    language="en", 
    model="mansa_v1",
	timestamps="sentence",#Choose either sentence-level transcriptions or word-level transcriptions
	special_words=["Spitch"] # Add your special words here
)
‚Äã
Response Format
The sample response format is:

Copy
SpeechTranscribeResponse(
    request_id="35580dcf-xxxx-4666-8d75-xxxxxxxxxx",
    text=(
        "I'm having some power issues, I won't be available for a bit. "
        "All right. Please let me know when you are back."
    ),
    timestamps=[
        Timestamp(
            start=1.20,
            end=4.64,
            text="I'm having some power issues, I won't be available for a bit."
        ),
        Timestamp(
            start=5.80,
            end=6.72,
            text="All right."
        ),
        Timestamp(
            start=8.08,
            end=9.92,
            text="Please let me know when you are back."
        ),
    ]
)
You can index into this response to fetch specific parameters like text or timestamps.
Mansa.v1 currently supports only English language. Additional language support will be available in upcoming releases.
‚Äã
Parameters
‚Äã
model
string
‚Äúmansa_v1‚Äù
‚Äã
timestamp
string
‚Äúsentence‚Äù, ‚Äúword‚Äù, ‚Äúnone‚Äù
‚Äã
special_words
comma separated list
You can use special_words to guide Mansa‚Äôs Named Entity Recognition. By entering in a list of entity strings, Mansa will accurately recognize and transcribe those terms in your audio.
Was this page helpful?


Yes

No
Previous
Sp
Speech Generation

Copy page

Also known as text-to-speech (TTS), speech generation/synthesis is integral part to modern AI systems. We have built this endpoint with strong support for African languages.

‚Äã
Request
The generate() function can be used to generate speech. Examples are provided below as a guide for you.
‚Äã
Best Practices for Use
We highly recommend that you perform tone-marking first before TTS. This allows the model to pronounce the words properly during speech generation.
Make sure your text has correct punctuation before sending it for speech generation to achieve more natural and accurate output.
Not all voices work for all languages. Ensure you select the voice that matches the language of your choice. More info on voices can be found on the Voices page
‚Äã
Response
The response for speech generation is in bytes.
The Content-Type is audio/wav
The content is streamed back to the caller.
The file type of the generated audio is wav. If you use the streaming interface (Python SDK), you can start to take action on the byte chunks, e.g. stream to file.
‚Äã
Choosing a Voice
We currently have 8 characters with unique voices for the supported languages. Each of these characters has unique attributes, we think you will find them fun to use. Feel free to try them out and let us know which one you love the most. üòâ
‚Äã
Language Support
The speech generation model supports the following languages:
English: en
Hausa: ha
Igbo: ig
Amharic: am
Yoruba: yo
‚Äã
Examples
Python


Copy
import os
from spitch import Spitch

os.environ["SPITCH_API_KEY"] = "YOUR_API_KEY"
client = Spitch()

with open("new.mp3", "wb") as f:
    response = client.speech.generate(
        text="Bawo ni ololufe mi?",
        language="yo",
        voice="sade"
    )
    f.write(response.read())
‚Äã
Examples - streaming
Python


Copy
import os
from spitch import Spitch

os.environ["SPITCH_API_KEY"] = "YOUR_API_KEY"
client = Spitch()

with client.speech.with_streaming_response.generate(
    text="Bawo ni ololufe mi?",
    language="yo",
    voice="funmi"
) as response:
    response.stream_to_file("new.wav")
Was this page helpful?


Yes

No
Previous
Transcription
Transcription

Copy page

Also known as speech-to-text (STT), transcription is the process of converting speech to text. We have built this endpoint with strong support for African languages.

‚Äã
Request
The transcribe() function can be used to transcribe audio. Pass either a url or a content to the transcribe function. Examples are provided below as a guide for you.
‚Äã
Best Practices for Use
You can provide either the content (file) or url (str), but do not provide both.
The maximum file size is 25MB, we will support larger sizes in the future.
We only support mp3, wav, m4a, and ogg file formats.
If you provide url, ensure that access to the file is not blocked by authentication.
When transcribing, you should use the language code (e.g. en, yo, ig) and not the full text.
‚Äã
Response
The response for speech generation is in bytes.
The Content-Type is application/json
A request_id is returned for issue resolution with our support team.
Below is an example of a response from the transcription endpoint.

Copy
    {
      "request_id": "86095cea-77d5-45ba-a093-0f800ac2c7df",
      "text": "B√°wo ni ol√≥l√πf·∫πÃÅ mi?"
    }
‚Äã
Language Support
Our speech-to-text model supports the following languages:
Hausa: ha
Igbo: ig
Yoruba: yo
Amharic: am
English: en
More info on languages can be found on the Languages page
‚Äã
Examples - file
Python


Copy
import os
from spitch import Spitch

os.environ["SPITCH_API_KEY"] = "YOUR_API_KEY"
client = Spitch()

with open("new.wav", "rb") as f:
    response = client.speech.transcribe(
        language="yo",
        content=f.read()
    )
print(f"Text: {response.text}")
‚Äã
Examples - url
Python


Copy
import os
from spitch import Spitch

os.environ["SPITCH_API_KEY"] = "YOUR_API_KEY"
client = Spitch()

response = client.speech.transcribe(
    language="yo",
    url="https://myfilelocation.com/file.mp3"
)
print(response.text)
Was this page helpful?


Yes

No
Previous
Translation

Copy page

Machine Translation converts text from one language to another. This is useful for any context that requires two languages.

‚Äã
Request
The translate() function can be used to transcribe audio. Pass a text, source and target languages. Examples are provided below as a guide for you.
‚Äã
Best Practices for Use
Context matters for many languages, so translations can differ based on wording and usage.
‚Äã
Language Support
Our speech-to-text model supports the following languages:
Hausa: ha
Igbo: ig
Yoruba: yo
English: en
More info on languages can be found on the Languages page
Python


Copy
from spitch import Spitch

os.environ["SPITCH_API_KEY"] = "YOUR_API_KEY"
client = Spitch()
translation = client.text.translate(
    text="Hey my dear friend, how are you doing?",
    source="en",
    target="ha",
)
print(translation.text)
Was this page helpful?


Yes

No
Previous
Tone Marking
Next
Tone Marking

Copy page

Many African languages are tonal (i.e. there is a specific way to pronounce them properly). In writing, this is indicated by certain marks which can help the reader (human or AI) to properly pronounce such text. We have built this endpoint with model support for African languages.

‚Äã
Request
The tone_mark() function can be used to add tone marks to a sentence. Examples are provided below as a guide for you.
‚Äã
Best Practices for Use
When tone marking a sentence, you should use the language code (e.g. yo, ig) and not the full text.
‚Äã
Response
The response for speech generation is in bytes.
The Content-Type is application/json
A request_id is returned for issue resolution with our support team.
Below is an example of a response from the API

Copy
{
  "request_id": "'86095cea-77d5-45ba-a093-0f800ac2c7df'",
  "text": "B√°wo ni ol√≥l√πf·∫πÃÅ mi?"
}
‚Äã
Language Support
The speech generation model supports the following languages:
Yoruba: yo
Support for Igbo is coming soon. More info on languages can be found on the Languages page
‚Äã
Examples
Python


Copy
import os
from spitch import Spitch

os.environ["SPITCH_API_KEY"] = "YOUR_API_KEY"
client = Spitch()
response = client.text.tone_mark(
    language="yo",
    text="Bawo ni ololufe mi?"
)
print(response.text)   # B√°wo ni ol√≥l√πf·∫πÃÅ mi?
Was this page helpful?


Yes

No
Previous
Overview
Welcome to the world of voice agents with Spitch!
Next
Powered by Mintlify
Overview

Copy page

Welcome to the world of voice agents with Spitch!

Voice agents are real-time systems that interact with users via spoken language. They often combine automatic speech recognition (ASR), natural language processing (NLP), text-to-speech (TTS), and even language translation to listen, understand, and respond to users in a conversational flow.
Voice agents:
listen to voice input using ASR (Automatic Speech Recognition)
understand user intent using NLP (could be an LLM or a rule-based system)
take action and return a reponse
respond with synthesized speech using TTS
‚Äã
The Voice Agent Pipeline
The typical voice agent is made up of the following steps:
VAD

ASR

NLU

LLM

TTS

VAD (Voice Activity Detection): This is responsible for detecting the precise moments when a user begins and stops speaking.
ASR (Automatic Speech Recognition): This feature transcribes input audio into text which is later passed on to the NLU system.
NLU (Natural Language Understanding): NLU examines the transcript to identify the user‚Äôs intent and extract key entities or parameters. This semantic analysis allows the agent to map user requests onto actionable commands or conversational flows.
LLM (Large Language Model): This element is responsible for generating the agent‚Äôs next utterance based on the interpreted intent from speech and the user‚Äôs conversation history.
TTS (Text-to-Speech): The TTS engine converts the agent‚Äôs chosen textual response into natural, human-like audio.
‚Äã
Why Use Voice Agents?
Voice Agents have many benefits to developers and businesses alike and can be applied to automate operations across many sectors. Some of the benefits are:
Enhanced Accessibility & Engagement
Voice Agents enable users with mobility or vision challenges to interact seamlessly with software.
Enable Hands-Free Interaction
It helps customers multitask in scenarios like driving or cooking, making your app safer and more convenient.
Automate High-Volume Workloads
Scale operations‚Äîhandle customer support tickets, bookings, and order processing‚Äîwithout adding headcount.
In the next section, we‚Äôll walk through how to build your own voice agent using Spitch SDKs.
Was this page helpful?


Yes

No
Previous
Livekit
Learn how to create voice agents on Livekit using Spitch STT and TTS models.
Next
Powered by Mintlify
On this page
The Voice Agent Pipeline
Voice Agents
Livekit

Copy page

Learn how to create voice agents on Livekit using Spitch STT and TTS models.

‚Äã
Prerequisites
Python 3.7 or higher
Spitch API key
Livekit API key, URL and secret key
‚Äã
Installation & Setup
Install the following modules into your Python environment and setup your *.env* file with the following parameters.

Copy
pip install "livekit-agents[spitch]~=1.0"


Copy
SPITCH_API_KEY=<Your Spitch API Key>
LIVEKIT_API_KEY=<your API Key>
LIVEKIT_API_SECRET=<your API Secret>
LIVEKIT_URL=<Your URL>
‚Äã
Integrating Spitch STT with Livekit

Copy
from livekit.plugins import spitch

session = AgentSession(
   stt=spitch.STT(
      language="en",
   ),
)
‚Äã
STT Parameters
‚Äã
language
stringrequired
Check here for language options
‚Äã
Integrating Spitch TTS with Livekit

Copy
from livekit.plugins import spitch

session = AgentSession(
   tts=spitch.TTS(
      language="en",
      voice="lina",
   )
)
‚Äã
TTS Parameters
‚Äã
language
stringdefault:"en"required
Check here for language options
‚Äã
voice
stringdefault:"en"required
Check here for voice options
‚Äã
Building a Voice Agent with Spitch
In this section, we would build an end-to-end voice agent using Spitch and Livekit. The flowchart below summarizes the architecture of our agent:
Audio Input

Spitch STT

LLM

Spitch TTS

Audio Output

To get started, you‚Äôll need API keys for Spitch, Livekit and your preffered LLM. We will be making use of OpenAI in this demo. To find out more about which LLMs are available on Livekit, check out this doc.
‚Äã
Install packages
Install the following packages on your local computer to get started.

Copy
pip install \
  "livekit-agents[spitch,openai,silero,turn-detector]~=1.0" \
  "livekit-plugins-noise-cancellation~=0.2" \
  "python-dotenv"
‚Äã
Set up your environment variables
.env

Copy
SPITCH_API_KEY=<Your Spitch API Key>
OPENAI_API_KEY=<Your OpenAI API Key>
LIVEKIT_API_KEY=<your API Key>
LIVEKIT_API_SECRET=<your API Secret>
LIVEKIT_URL=<Your Livekit URL>
‚Äã
Voice Agent Code
agent.py

Copy
from dotenv import load_dotenv
from livekit import agents
from livekit.agents import AgentSession, Agent, RoomInputOptions
from livekit.plugins import (
   spitch,
    openai,
    noise_cancellation,
    silero,
)
from livekit.plugins.turn_detector.multilingual import MultilingualModel

load_dotenv()

class Assistant(Agent):
    def __init__(self) -> None:
        super().__init__(instructions="You are a helpful voice AI assistant.") #You can change the instrruction to meet your unique use case.

async def entrypoint(ctx: agents.JobContext):
    session = AgentSession(
        stt=spitch.STT(language="en"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=spitch.TTS(language="en", voice="lina")
        vad=silero.VAD.load(), #Voice Activity Detection Model
        turn_detection=MultilingualModel(), #Turn Detection Model
    )

    await session.start(
        room=ctx.room,
        agent=Assistant(),
        room_input_options=RoomInputOptions(
            noise_cancellation=noise_cancellation.BVC(), 
        ),
    )

    await session.generate_reply(
        instructions="Greet the user and offer your assistance."
    )


if __name__ == "__main__":
    agents.cli.run_app(agents.WorkerOptions(entrypoint_fnc=entrypoint))
‚Äã
Running the Agent
Download model files

Download the model files first to use the turn-detector, silero, or noise-cancellation plugins:


Copy
python agent.py download-files
Speak to the agent

Start your agent in console mode to run inside your terminal:


Copy
python agent.py console
Connect the agent to the playground

Start the agent in dev mode to connect it to LiveKit and make it available from anywhere on the internet:


Copy
python agent.py dev
‚Äã
Additional Resources
Integrating Spitch TTS in a Livekit voice agent
Integrating Spitch STT in a Livekit voice agent
Was this page helpful?


Yes

No
Previous
Audio Transcription App
Next
Powered by Mintlify
On this page
Prerequisites
Installation & Setup
Integrating Spitch STT with Livekit
STT Parameters
Integrating Spitch TTS with Livekit
TTS Parameters
Building a Voice Agent with Spitch
Install packages
Set up your environment variables
Voice Agent Code
Running the Agent
Audio Transcription App

Copy page

This guide walks you through setting up and running a basic audio transcription app using the Spitch Python SDK and Streamlit.
‚Äã
Overview
This app allows users to upload audio files (WAV, OGG, M4A or MP3), select a language, and receive an accurate transcription using the Spitch ASR API.
‚Äã
üîß Prerequisites
Before you start, make sure you have:
Python 3.7+
Streamlit
Spitch Python SDK
A Spitch API key
‚Äã
üöÄ Installation
Install the required packages:

Copy
pip install streamlit spitch
‚Äã
üß† App Structure
Your main application code lives in a single file. Here‚Äôs a breakdown of its structure:
‚Äã
1. Import Modules

Copy
import streamlit as st
from spitch import Spitch
import os
import tempfile
‚Äã
2. Transcription Logic

Copy
def transcribe_audio(audio_file, lang):
    os.environ["SPITCH_API_KEY"] = "YOUR-API-KEY"
    client = Spitch()

    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_file:
        temp_file.write(audio_file.read())
        temp_path = temp_file.name

    with open(temp_path, "rb") as f:
        response = client.speech.transcribe(language=lang, content=f.read())

    return response.text
‚Äã
3. Streamlit UI

Copy
def main():
    st.title("My Spitch Transcription App")
    st.write("Upload an audio file to transcribe it to text.")

    audio_file = st.file_uploader("Upload Audio", type=["wav", "mp3"])

    if audio_file:
        st.audio(audio_file, format='audio/wav')
        language = st.selectbox("Language", ["English", "Yoruba", "Igbo", "Hausa"])
        lang = {'English': 'en', 'Yoruba': 'yo', 'Igbo': 'ig'}.get(language, 'ha')

        if st.button("Transcribe"):
            with st.spinner("Transcribing..."):
                transcript = transcribe_audio(audio_file, lang)
                st.success("Transcription completed!")
                st.text_area("Transcript", transcript, height=200)
                st.download_button("Download Transcript", data=transcript, file_name="transcript.txt")

if __name__ == "__main__":
    main()
‚Äã
üßë‚Äçüíª Access the code here

Copy
$ git clone https://github.com/Nalito/My-Spitch-Transcription-App.git
$ cd My-Spitch-Transcription-App
‚Äã
üß™ Testing the App
Run the Streamlit app:

Copy
streamlit run app.py
Upload an audio file, select the language, and click Transcribe to test the transcription.
‚Äã
üõ†Ô∏è Make it your own!
Play around with the code to store transcripts in databases, or even integrate with frontend frameworks for a richer UI!
Was this page helpful?


Yes

No
Previous
Multilingual Chatbot
Next
Code Walkthroughs
Multilingual Chatbot

Copy page

This guide walks you through building a simple multilingual FAQ chatbot using the Spitch API.
‚Äã
üîß Prerequisites
Before you start, make sure you have:
Python 3.7+
Spitch Python SDK
A Spitch API key
‚Äã
üöÄ Installation
Install the required packages:

Copy
pip install spitch
‚Äã
Code Walkthrough
‚Äã
1. Import and Setup
We import the necessary modules, set the API key, and initialize the Spitch client.

Copy
import os
from spitch import Spitch

# Set up Spitch client
os.environ["SPITCH_API_KEY"] = "your_api_key_here"
client = Spitch()
‚Äã
2. FAQ Database
We define a simple English-based FAQ dictionary.

Copy
faq_db = {
    "hi": "hello. how may I help you",
    "what are your opening hours": "Our support team is available 24/7 to assist you.",
    "how can i reset my password": "To reset your password, click on 'Forgot Password' on the login page.",
    # ... more FAQs here
}
‚Äã
3. Translation Function
This helper function uses Spitch‚Äôs text translation service.

Copy
def translate(text, source_lang, target_lang):
    translation = client.text.translate(
        text=text,
        source=source_lang,
        target=target_lang,
    )
    return translation.text.lower()
‚Äã
4. FAQ Matching
Matches the normalized user input against the FAQ database.

Copy
def get_response(user_input):
    normalized = user_input.strip().lower()
    for question, answer in faq_db.items():
        if question in normalized:
            return answer
    return "I'm sorry, I didn't understand that. Could you please rephrase?"
‚Äã
5. Main Chat Loop
This is where the chatbot runs interactively.

Copy
def main():
    print("Welcome to Multilingual Support Chatbot!")
    source_lang = input("Enter your language code (e.g., 'yo' for Yoruba, 'ig' for Igbo): ")

    while True:
        user_input = input("You: ")
        if user_input.lower() in ["exit", "quit"]:
            print("Goodbye!")
            break

        # Translate to English for matching
        input_en = translate(user_input, source_lang, "en")

        # Get English response
        response_en = get_response(input_en)

        # Translate back to user's language
        response_user_lang = translate(response_en, "en", source_lang)
        print(f"Bot: {response_user_lang}")

if __name__ == "__main__":
    main()
‚Äã
üßë‚Äçüíª Access the code here

Copy
$ git clone https://github.com/Nalito/spitch-multilingual-chatbot-python.git
$ cd spitch-multilingual-chatbot-python
‚Äã
Running the Chatbot
Run this command in your terminal:

Copy
python chatbot.py
Enter your preferred language code, ask a question, and the bot will respond in your chosen language.
‚Äã
Example Interaction

Copy
Welcome to Multilingual Support Chatbot!
Enter your language code (e.g., 'yo' for Yoruba, 'ig' for Igbo): yo
You: Bawo ni mo se le tun oro igbaniwo mi se
Bot: lati tun oro igbaniwo re se, t·∫π lori 'gbagbe oro igbaniwo' lori oju-iwe wiw·ªçle ati t·∫πle aw·ªçn ilana naa.
‚Äã
Next Steps
Expand the faq_db with more multilingual entries.
Integrate with a web UI or messaging platform.
Add speech-to-text and text-to-speech for full voice-based interaction.
Was this page helpful?


Yes

No
Previous
Voices
Next
Concepts
Voices

Copy page

Voices are how we generate speech. Selecting the right voice can help you convey the tone and emotions you desire.
Find details about our voices below.
Voice	Language	Voice Type	Description
Sade	Yoruba	Feminine	Energectic, but breezy
Funmi	Yoruba	Feminine	Calm, can sometimes be fun.
Segun	Yoruba	Masculine	Vibrant, yet cool.
Femi	Yoruba	Masculine	Really fun guy to interact with.
Hasan	Hausa	Masculine	Loud and clear voice
Amina	Hausa	Feminine	A bit quiet and soft.
Zainab	Hausa	Feminine	Clear, loud voice.
Aliyu	Hausa	Masculine	Soft voice, cool tone.
Obinna	Igbo	Masculine	Loud and clear voice
Ngozi	Igbo	Feminine	A bit quiet and soft.
Amara	Igbo	Feminine	Clear, loud voice.
Ebuka	Igbo	Masculine	Soft voice, cool tone.
John	English	Masculine	Loud and clear voice
Lucy	English	Feminine	Very clear voice.
Lina	English	Feminine	Clear, loud voice.
Jude	English	Masculine	Deep voice, smooth.
Henry	English	Masculine	Soft voice, cool tone.
Kani	English	Femini	Soft voice, cool tone.
Hana	Amharic	Feminine	
Selam	Amharic	Masculine	
Tesfaye	Amharic	Masculine	
Tena	Amharic	Feminine	
More voices are coming to our platform very soon.
Was this page helpful?


Yes

No
Previous
Languages
Next
Concepts
Languages

Copy page

Voices are how we generate speech. Selecting the right voice can help you convey the tone and emotions.
Always use the short code when referencing a language while calling the API.
Find supported features for the languages below.
Language	Short code	STT	TTS	Tone Mark
Hausa	ha			
Igbo	ig			
Yoruba	yo			
English	en			
Amharic	am			
Full Support for Igbo and Hausa are coming soon.
More languages are coming to our platform very soon.
Reach out to the support team for access to Amharic.
Was this page helpful?


Yes

No
Previous
Pricing
Next
Powered by Mintlify
Pricing

Copy page

‚Äã
API Pricing
As a new developer, you get $1 free credit to explore the Spitch API. This allows you to perform;
12.5 minutes of speech generation
40 minutes of transcriptions
Find our pricing in the table below:
Feature	Pricing
Speech Generation	$0.08 per minute
Transcription	$0.025 per minute
Tone Marking	$1 per 10,000 words
Translation	$1 per 10,000 words
We‚Äôre working hard to lower these prices even further.
Was this page helpful?


Yes

No
Previous
Languages
Powered by Mintlify
On this page
